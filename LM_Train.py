# import argparse
import time
import math
import os
import torch
import torch.nn as nn
import torch.onnx

from utils import data
from models.RNNLM import RNNModel

## params
batch_size = 64
eval_batch_size = 10
bptt = 30
lr = 0.1
clip = 0.25
log_interval = 10
epochs = 10
##

if torch.cuda.is_available():
	device = torch.device("cuda")
else:
	device = torch.device("cpu")

corpus = data.Corpus('./wikitext')

print(corpus.valid.shape)
print(corpus.valid.size())

def batchify(data, bsz):
	# Work out how cleanly we can divide the dataset into bsz parts.
	nbatch = data.size(0) // bsz
	# Trim off any extra elements that wouldn't cleanly fit (remainders).
	data = data.narrow(0, 0, nbatch * bsz)
	# Evenly divide the data across the bsz batches.
	data = data.view(bsz, -1).t().contiguous()
	return data.to(device)


train_data = batchify(corpus.train, batch_size)
val_data = batchify(corpus.valid, eval_batch_size)
test_data = batchify(corpus.test, eval_batch_size)

def repackage_hidden(h):
	"""Wraps hidden states in new Tensors, to detach them from their history."""
	if isinstance(h, torch.Tensor):
		return h.detach()
	else:
		return tuple(repackage_hidden(v) for v in h)

def get_batch(source, i):
	seq_len = min(bptt, len(source) - 1 - i)
	data = source[i:i+seq_len]
	target = source[i+1:i+1+seq_len].view(-1)
	return data, target

def evaluate(data_source):
	# Turn on evaluation mode which disables dropout.
	model.eval()
	total_loss = 0.
	ntokens = len(corpus.dictionary)
	hidden = model.init_hidden(eval_batch_size)
	with torch.no_grad():
		for i in range(0, data_source.size(0) - 1, bptt):
			data, targets = get_batch(data_source, i)
			output, hidden = model(data, hidden)
			output_flat = output.view(-1, ntokens)
			total_loss += len(data) * criterion(output_flat, targets).item()
			hidden = repackage_hidden(hidden)
	return total_loss / (len(data_source) - 1)

def train():
	# Turn on training mode which enables dropout.
	model.train()
	total_loss = 0.
	start_time = time.time()
	ntokens = len(corpus.dictionary)
	hidden = model.init_hidden(batch_size)
	for batch, i in enumerate(range(0, train_data.size(0) - 1, bptt)):
		data, targets = get_batch(train_data, i)
		# Starting each batch, we detach the hidden state from how it was previously produced.
		# If we didn't, the model would try backpropagating all the way to start of the dataset.
		hidden = repackage_hidden(hidden)
		model.zero_grad()
		output, hidden = model(data, hidden)
		loss = criterion(output.view(-1, ntokens), targets)
		loss.backward()

		# `clip_grad_norm` helps prevent the exploding gradient problem in RNNs / LSTMs.
		torch.nn.utils.clip_grad_norm_(model.parameters(), clip)
		for p in model.parameters():
			p.data.add_(-lr, p.grad.data)

		total_loss += loss.item()

		if batch % log_interval == 0 and batch > 0:
			cur_loss = total_loss / log_interval
			elapsed = time.time() - start_time
			print('| epoch {:3d} | {:5d}/{:5d} batches | lr {:02.2f} | ms/batch {:5.2f} | '
					'loss {:5.2f} | ppl {:8.2f}'.format(
				epoch, batch, len(train_data) // bptt, lr,
				elapsed * 1000 / log_interval, cur_loss, math.exp(cur_loss)))
			total_loss = 0
	start_time = time.time()

ntokens = len(corpus.dictionary)
model = RNNModel(ntokens).to(device)

criterion = nn.CrossEntropyLoss()

best_val_loss = None

try:
	for epoch in range(1, epochs+1):
		epoch_start_time = time.time()
		train()
		val_loss = evaluate(val_data)
		print('-' * 89)
		print('| end of epoch {:3d} | time: {:5.2f}s | valid loss {:5.2f} | '
				'valid ppl {:8.2f}'.format(epoch, (time.time() - epoch_start_time),
										   val_loss, math.exp(val_loss)))
		print('-' * 89)
		# Save the model if the validation loss is the best we've seen so far.
		if not best_val_loss or val_loss < best_val_loss:
			# with open(save, 'wb') as f:
			#     torch.save(model, f)
			best_val_loss = val_loss
		else:
			# Anneal the learning rate if no improvement has been seen in the validation dataset.
			lr /= 4.0
except KeyboardInterrupt:
	print('-' * 89)
	print('Exiting from training early')