{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The autoreload extension is already loaded. To reload it, use:\n",
      "  %reload_ext autoreload\n"
     ]
    }
   ],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import argparse\n",
    "import time\n",
    "import math\n",
    "import os\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.onnx\n",
    "\n",
    "from utils import data\n",
    "from model.RNNModel import RNNModel"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "device = torch.device(\"cpu\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "corpus = data.Corpus('./data/wikitext')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([217646])\n",
      "torch.Size([217646])\n"
     ]
    }
   ],
   "source": [
    "print(corpus.valid.shape)\n",
    "print(corpus.valid.size())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "def batchify(data, bsz):\n",
    "    # Work out how cleanly we can divide the dataset into bsz parts.\n",
    "    nbatch = data.size(0) // bsz\n",
    "    # Trim off any extra elements that wouldn't cleanly fit (remainders).\n",
    "    data = data.narrow(0, 0, nbatch * bsz)\n",
    "    # Evenly divide the data across the bsz batches.\n",
    "    data = data.view(bsz, -1).t().contiguous()\n",
    "    return data.to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_data = batchify(corpus.train, batch_size)\n",
    "val_data = batchify(corpus.valid, eval_batch_size)\n",
    "test_data = batchify(corpus.test, eval_batch_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "def repackage_hidden(h):\n",
    "    \"\"\"Wraps hidden states in new Tensors, to detach them from their history.\"\"\"\n",
    "    if isinstance(h, torch.Tensor):\n",
    "        return h.detach()\n",
    "    else:\n",
    "        return tuple(repackage_hidden(v) for v in h)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_batch(source, i):\n",
    "    seq_len = min(bptt, len(source) - 1 - i)\n",
    "    data = source[i:i+seq_len]\n",
    "    target = source[i+1:i+1+seq_len].view(-1)\n",
    "    return data, target"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate(data_source):\n",
    "    # Turn on evaluation mode which disables dropout.\n",
    "    model.eval()\n",
    "    total_loss = 0.\n",
    "    ntokens = len(corpus.dictionary)\n",
    "    hidden = model.init_hidden(eval_batch_size)\n",
    "    with torch.no_grad():\n",
    "        for i in range(0, data_source.size(0) - 1, bptt):\n",
    "            data, targets = get_batch(data_source, i)\n",
    "            output, hidden = model(data, hidden)\n",
    "            output_flat = output.view(-1, ntokens)\n",
    "            total_loss += len(data) * criterion(output_flat, targets).item()\n",
    "            hidden = repackage_hidden(hidden)\n",
    "    return total_loss / (len(data_source) - 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train():\n",
    "    # Turn on training mode which enables dropout.\n",
    "    model.train()\n",
    "    total_loss = 0.\n",
    "    start_time = time.time()\n",
    "    ntokens = len(corpus.dictionary)\n",
    "    hidden = model.init_hidden(batch_size)\n",
    "    for batch, i in enumerate(range(0, train_data.size(0) - 1, bptt)):\n",
    "        data, targets = get_batch(train_data, i)\n",
    "        # Starting each batch, we detach the hidden state from how it was previously produced.\n",
    "        # If we didn't, the model would try backpropagating all the way to start of the dataset.\n",
    "        hidden = repackage_hidden(hidden)\n",
    "        model.zero_grad()\n",
    "        output, hidden = model(data, hidden)\n",
    "        loss = criterion(output.view(-1, ntokens), targets)\n",
    "        loss.backward()\n",
    "\n",
    "        # `clip_grad_norm` helps prevent the exploding gradient problem in RNNs / LSTMs.\n",
    "        torch.nn.utils.clip_grad_norm_(model.parameters(), clip)\n",
    "        for p in model.parameters():\n",
    "            p.data.add_(-lr, p.grad.data)\n",
    "\n",
    "        total_loss += loss.item()\n",
    "\n",
    "        if batch % log_interval == 0 and batch > 0:\n",
    "            cur_loss = total_loss / log_interval\n",
    "            elapsed = time.time() - start_time\n",
    "            print('| epoch {:3d} | {:5d}/{:5d} batches | lr {:02.2f} | ms/batch {:5.2f} | '\n",
    "                    'loss {:5.2f} | ppl {:8.2f}'.format(\n",
    "                epoch, batch, len(train_data) // bptt, lr,\n",
    "                elapsed * 1000 / log_interval, cur_loss, math.exp(cur_loss)))\n",
    "            total_loss = 0\n",
    "    start_time = time.time()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_size = 64\n",
    "eval_batch_size = 10\n",
    "bptt = 30\n",
    "lr = 0.1\n",
    "clip = 0.25\n",
    "log_interval = 10\n",
    "epochs = 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {},
   "outputs": [],
   "source": [
    "ntokens = len(corpus.dictionary)\n",
    "model = RNNModel(ntokens).to(device)\n",
    "\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "\n",
    "best_val_loss = None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "| epoch   1 |    10/ 1087 batches | lr 0.10 | ms/batch 2297.50 | loss 12.48 | ppl 262476.24\n",
      "| epoch   1 |    20/ 1087 batches | lr 0.10 | ms/batch 4424.05 | loss 11.33 | ppl 83098.57\n",
      "| epoch   1 |    30/ 1087 batches | lr 0.10 | ms/batch 6633.24 | loss 11.31 | ppl 81952.56\n",
      "| epoch   1 |    40/ 1087 batches | lr 0.10 | ms/batch 8739.09 | loss 11.30 | ppl 80786.68\n",
      "| epoch   1 |    50/ 1087 batches | lr 0.10 | ms/batch 11029.83 | loss 11.28 | ppl 79533.09\n",
      "| epoch   1 |    60/ 1087 batches | lr 0.10 | ms/batch 13139.33 | loss 11.27 | ppl 78558.06\n",
      "| epoch   1 |    70/ 1087 batches | lr 0.10 | ms/batch 15221.36 | loss 11.26 | ppl 77353.32\n",
      "| epoch   1 |    80/ 1087 batches | lr 0.10 | ms/batch 17315.48 | loss 11.24 | ppl 76134.95\n",
      "| epoch   1 |    90/ 1087 batches | lr 0.10 | ms/batch 19420.89 | loss 11.22 | ppl 74956.49\n",
      "| epoch   1 |   100/ 1087 batches | lr 0.10 | ms/batch 21538.03 | loss 11.21 | ppl 74060.50\n",
      "| epoch   1 |   110/ 1087 batches | lr 0.10 | ms/batch 23622.86 | loss 11.19 | ppl 72627.64\n",
      "| epoch   1 |   120/ 1087 batches | lr 0.10 | ms/batch 25718.24 | loss 11.17 | ppl 71163.02\n",
      "| epoch   1 |   130/ 1087 batches | lr 0.10 | ms/batch 27816.21 | loss 11.15 | ppl 69715.83\n",
      "| epoch   1 |   140/ 1087 batches | lr 0.10 | ms/batch 29921.82 | loss 11.13 | ppl 68110.37\n",
      "| epoch   1 |   150/ 1087 batches | lr 0.10 | ms/batch 31998.23 | loss 11.11 | ppl 66811.42\n",
      "| epoch   1 |   160/ 1087 batches | lr 0.10 | ms/batch 34084.57 | loss 11.09 | ppl 65338.71\n",
      "| epoch   1 |   170/ 1087 batches | lr 0.10 | ms/batch 36174.33 | loss 11.06 | ppl 63495.11\n",
      "| epoch   1 |   180/ 1087 batches | lr 0.10 | ms/batch 38287.12 | loss 11.04 | ppl 62345.44\n",
      "| epoch   1 |   190/ 1087 batches | lr 0.10 | ms/batch 40374.59 | loss 11.01 | ppl 60386.15\n",
      "| epoch   1 |   200/ 1087 batches | lr 0.10 | ms/batch 42517.48 | loss 10.98 | ppl 58455.15\n",
      "| epoch   1 |   210/ 1087 batches | lr 0.10 | ms/batch 44585.68 | loss 10.96 | ppl 57391.01\n",
      "| epoch   1 |   220/ 1087 batches | lr 0.10 | ms/batch 46619.45 | loss 10.91 | ppl 54503.86\n",
      "| epoch   1 |   230/ 1087 batches | lr 0.10 | ms/batch 48662.87 | loss 10.87 | ppl 52719.36\n",
      "| epoch   1 |   240/ 1087 batches | lr 0.10 | ms/batch 50712.56 | loss 10.82 | ppl 50128.48\n",
      "| epoch   1 |   250/ 1087 batches | lr 0.10 | ms/batch 52742.49 | loss 10.77 | ppl 47790.31\n",
      "| epoch   1 |   260/ 1087 batches | lr 0.10 | ms/batch 54788.98 | loss 10.73 | ppl 45763.59\n",
      "| epoch   1 |   270/ 1087 batches | lr 0.10 | ms/batch 56821.97 | loss 10.66 | ppl 42676.81\n",
      "| epoch   1 |   280/ 1087 batches | lr 0.10 | ms/batch 58854.33 | loss 10.62 | ppl 40896.87\n",
      "| epoch   1 |   290/ 1087 batches | lr 0.10 | ms/batch 60888.49 | loss 10.56 | ppl 38666.39\n",
      "| epoch   1 |   300/ 1087 batches | lr 0.10 | ms/batch 62916.26 | loss 10.51 | ppl 36608.04\n",
      "| epoch   1 |   310/ 1087 batches | lr 0.10 | ms/batch 64937.01 | loss 10.42 | ppl 33368.74\n",
      "| epoch   1 |   320/ 1087 batches | lr 0.10 | ms/batch 66967.87 | loss 10.35 | ppl 31148.48\n",
      "| epoch   1 |   330/ 1087 batches | lr 0.10 | ms/batch 69001.10 | loss 10.28 | ppl 29255.03\n",
      "| epoch   1 |   340/ 1087 batches | lr 0.10 | ms/batch 71030.96 | loss 10.22 | ppl 27491.16\n",
      "| epoch   1 |   350/ 1087 batches | lr 0.10 | ms/batch 73072.88 | loss 10.15 | ppl 25672.57\n",
      "| epoch   1 |   360/ 1087 batches | lr 0.10 | ms/batch 75097.91 | loss 10.08 | ppl 23879.04\n",
      "| epoch   1 |   370/ 1087 batches | lr 0.10 | ms/batch 77146.55 | loss 10.04 | ppl 22998.76\n",
      "| epoch   1 |   380/ 1087 batches | lr 0.10 | ms/batch 79186.16 | loss 10.00 | ppl 22089.81\n",
      "| epoch   1 |   390/ 1087 batches | lr 0.10 | ms/batch 81219.47 | loss  9.94 | ppl 20738.63\n",
      "| epoch   1 |   400/ 1087 batches | lr 0.10 | ms/batch 83260.35 | loss  9.93 | ppl 20535.83\n",
      "| epoch   1 |   410/ 1087 batches | lr 0.10 | ms/batch 85295.32 | loss  9.88 | ppl 19469.19\n",
      "| epoch   1 |   420/ 1087 batches | lr 0.10 | ms/batch 87327.85 | loss  9.83 | ppl 18610.39\n",
      "| epoch   1 |   430/ 1087 batches | lr 0.10 | ms/batch 89360.57 | loss  9.83 | ppl 18588.17\n",
      "| epoch   1 |   440/ 1087 batches | lr 0.10 | ms/batch 91397.39 | loss  9.79 | ppl 17795.19\n",
      "| epoch   1 |   450/ 1087 batches | lr 0.10 | ms/batch 93437.02 | loss  9.80 | ppl 17951.26\n",
      "| epoch   1 |   460/ 1087 batches | lr 0.10 | ms/batch 95466.05 | loss  9.77 | ppl 17496.48\n",
      "| epoch   1 |   470/ 1087 batches | lr 0.10 | ms/batch 97503.90 | loss  9.71 | ppl 16526.20\n",
      "| epoch   1 |   480/ 1087 batches | lr 0.10 | ms/batch 99537.06 | loss  9.73 | ppl 16808.92\n",
      "| epoch   1 |   490/ 1087 batches | lr 0.10 | ms/batch 101566.95 | loss  9.68 | ppl 15944.45\n",
      "| epoch   1 |   500/ 1087 batches | lr 0.10 | ms/batch 103598.41 | loss  9.64 | ppl 15370.90\n",
      "| epoch   1 |   510/ 1087 batches | lr 0.10 | ms/batch 105647.37 | loss  9.63 | ppl 15253.64\n",
      "| epoch   1 |   520/ 1087 batches | lr 0.10 | ms/batch 107705.37 | loss  9.63 | ppl 15150.81\n",
      "| epoch   1 |   530/ 1087 batches | lr 0.10 | ms/batch 109729.30 | loss  9.60 | ppl 14785.16\n",
      "| epoch   1 |   540/ 1087 batches | lr 0.10 | ms/batch 111763.69 | loss  9.51 | ppl 13514.59\n",
      "| epoch   1 |   550/ 1087 batches | lr 0.10 | ms/batch 113799.41 | loss  9.54 | ppl 13839.86\n",
      "| epoch   1 |   560/ 1087 batches | lr 0.10 | ms/batch 115825.43 | loss  9.51 | ppl 13546.07\n",
      "| epoch   1 |   570/ 1087 batches | lr 0.10 | ms/batch 117858.83 | loss  9.52 | ppl 13580.92\n",
      "| epoch   1 |   580/ 1087 batches | lr 0.10 | ms/batch 119898.19 | loss  9.51 | ppl 13500.14\n",
      "| epoch   1 |   590/ 1087 batches | lr 0.10 | ms/batch 121925.00 | loss  9.48 | ppl 13042.37\n",
      "| epoch   1 |   600/ 1087 batches | lr 0.10 | ms/batch 123953.81 | loss  9.43 | ppl 12410.23\n",
      "| epoch   1 |   610/ 1087 batches | lr 0.10 | ms/batch 126003.06 | loss  9.44 | ppl 12558.43\n",
      "| epoch   1 |   620/ 1087 batches | lr 0.10 | ms/batch 128032.73 | loss  9.42 | ppl 12285.44\n",
      "| epoch   1 |   630/ 1087 batches | lr 0.10 | ms/batch 130059.65 | loss  9.41 | ppl 12264.53\n",
      "| epoch   1 |   640/ 1087 batches | lr 0.10 | ms/batch 132085.56 | loss  9.42 | ppl 12283.86\n",
      "| epoch   1 |   650/ 1087 batches | lr 0.10 | ms/batch 134128.81 | loss  9.39 | ppl 11977.92\n",
      "| epoch   1 |   660/ 1087 batches | lr 0.10 | ms/batch 136160.61 | loss  9.41 | ppl 12157.30\n",
      "| epoch   1 |   670/ 1087 batches | lr 0.10 | ms/batch 138193.48 | loss  9.37 | ppl 11677.87\n",
      "| epoch   1 |   680/ 1087 batches | lr 0.10 | ms/batch 140214.28 | loss  9.31 | ppl 11045.11\n",
      "| epoch   1 |   690/ 1087 batches | lr 0.10 | ms/batch 142248.83 | loss  9.37 | ppl 11719.00\n",
      "| epoch   1 |   700/ 1087 batches | lr 0.10 | ms/batch 144277.52 | loss  9.32 | ppl 11124.66\n",
      "| epoch   1 |   710/ 1087 batches | lr 0.10 | ms/batch 146304.30 | loss  9.36 | ppl 11590.83\n",
      "| epoch   1 |   720/ 1087 batches | lr 0.10 | ms/batch 148396.11 | loss  9.38 | ppl 11835.93\n",
      "| epoch   1 |   730/ 1087 batches | lr 0.10 | ms/batch 150490.46 | loss  9.31 | ppl 11045.35\n",
      "| epoch   1 |   740/ 1087 batches | lr 0.10 | ms/batch 152591.80 | loss  9.33 | ppl 11319.03\n",
      "| epoch   1 |   750/ 1087 batches | lr 0.10 | ms/batch 154682.60 | loss  9.34 | ppl 11355.60\n",
      "| epoch   1 |   760/ 1087 batches | lr 0.10 | ms/batch 157024.15 | loss  9.30 | ppl 10953.91\n",
      "| epoch   1 |   770/ 1087 batches | lr 0.10 | ms/batch 159170.12 | loss  9.25 | ppl 10363.05\n",
      "| epoch   1 |   780/ 1087 batches | lr 0.10 | ms/batch 161302.63 | loss  9.24 | ppl 10280.28\n",
      "| epoch   1 |   790/ 1087 batches | lr 0.10 | ms/batch 163442.12 | loss  9.23 | ppl 10212.55\n",
      "| epoch   1 |   800/ 1087 batches | lr 0.10 | ms/batch 165574.39 | loss  9.26 | ppl 10548.56\n",
      "| epoch   1 |   810/ 1087 batches | lr 0.10 | ms/batch 167821.39 | loss  9.23 | ppl 10231.23\n",
      "| epoch   1 |   820/ 1087 batches | lr 0.10 | ms/batch 169963.77 | loss  9.24 | ppl 10333.77\n",
      "| epoch   1 |   830/ 1087 batches | lr 0.10 | ms/batch 172189.53 | loss  9.22 | ppl 10084.74\n",
      "| epoch   1 |   840/ 1087 batches | lr 0.10 | ms/batch 174714.73 | loss  9.23 | ppl 10192.21\n",
      "| epoch   1 |   850/ 1087 batches | lr 0.10 | ms/batch 177144.98 | loss  9.22 | ppl 10061.50\n",
      "| epoch   1 |   860/ 1087 batches | lr 0.10 | ms/batch 179490.80 | loss  9.19 | ppl  9841.57\n",
      "| epoch   1 |   870/ 1087 batches | lr 0.10 | ms/batch 181667.01 | loss  9.21 | ppl 10021.54\n",
      "| epoch   1 |   880/ 1087 batches | lr 0.10 | ms/batch 183983.66 | loss  9.18 | ppl  9698.42\n",
      "| epoch   1 |   890/ 1087 batches | lr 0.10 | ms/batch 186164.03 | loss  9.16 | ppl  9553.10\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "| epoch   1 |   900/ 1087 batches | lr 0.10 | ms/batch 188396.80 | loss  9.13 | ppl  9246.30\n",
      "| epoch   1 |   910/ 1087 batches | lr 0.10 | ms/batch 190552.93 | loss  9.15 | ppl  9389.24\n",
      "| epoch   1 |   920/ 1087 batches | lr 0.10 | ms/batch 192676.07 | loss  9.20 | ppl  9930.50\n",
      "| epoch   1 |   930/ 1087 batches | lr 0.10 | ms/batch 194875.73 | loss  9.16 | ppl  9493.79\n",
      "| epoch   1 |   940/ 1087 batches | lr 0.10 | ms/batch 197022.25 | loss  9.12 | ppl  9104.87\n",
      "| epoch   1 |   950/ 1087 batches | lr 0.10 | ms/batch 199170.11 | loss  9.07 | ppl  8712.15\n",
      "| epoch   1 |   960/ 1087 batches | lr 0.10 | ms/batch 201270.12 | loss  9.09 | ppl  8900.62\n",
      "| epoch   1 |   970/ 1087 batches | lr 0.10 | ms/batch 203544.54 | loss  9.13 | ppl  9237.01\n",
      "| epoch   1 |   980/ 1087 batches | lr 0.10 | ms/batch 205710.41 | loss  9.15 | ppl  9458.77\n",
      "| epoch   1 |   990/ 1087 batches | lr 0.10 | ms/batch 207829.86 | loss  9.08 | ppl  8794.52\n",
      "| epoch   1 |  1000/ 1087 batches | lr 0.10 | ms/batch 210005.32 | loss  9.05 | ppl  8500.51\n",
      "| epoch   1 |  1010/ 1087 batches | lr 0.10 | ms/batch 212169.68 | loss  9.08 | ppl  8745.78\n",
      "| epoch   1 |  1020/ 1087 batches | lr 0.10 | ms/batch 214372.57 | loss  9.06 | ppl  8597.91\n",
      "| epoch   1 |  1030/ 1087 batches | lr 0.10 | ms/batch 216599.59 | loss  9.09 | ppl  8856.73\n",
      "| epoch   1 |  1040/ 1087 batches | lr 0.10 | ms/batch 218791.80 | loss  9.06 | ppl  8604.23\n",
      "| epoch   1 |  1050/ 1087 batches | lr 0.10 | ms/batch 220941.86 | loss  9.09 | ppl  8873.56\n",
      "| epoch   1 |  1060/ 1087 batches | lr 0.10 | ms/batch 223162.51 | loss  9.07 | ppl  8716.11\n",
      "| epoch   1 |  1070/ 1087 batches | lr 0.10 | ms/batch 225294.86 | loss  9.06 | ppl  8600.75\n",
      "| epoch   1 |  1080/ 1087 batches | lr 0.10 | ms/batch 227457.03 | loss  9.04 | ppl  8444.26\n",
      "-----------------------------------------------------------------------------------------\n",
      "| end of epoch   1 | time: 2416.64s | valid loss  8.87 | valid ppl  7104.14\n",
      "-----------------------------------------------------------------------------------------\n"
     ]
    },
    {
     "ename": "NameError",
     "evalue": "name 'best_val_loss' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-70-60b9bc0654a8>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     10\u001b[0m         \u001b[0;32mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'-'\u001b[0m \u001b[0;34m*\u001b[0m \u001b[0;36m89\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     11\u001b[0m         \u001b[0;31m# Save the model if the validation loss is the best we've seen so far.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 12\u001b[0;31m         \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mbest_val_loss\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0mval_loss\u001b[0m \u001b[0;34m<\u001b[0m \u001b[0mbest_val_loss\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     13\u001b[0m             \u001b[0;32mwith\u001b[0m \u001b[0mopen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msave\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'wb'\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mf\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     14\u001b[0m                 \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msave\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mf\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mNameError\u001b[0m: name 'best_val_loss' is not defined"
     ]
    }
   ],
   "source": [
    "try:\n",
    "    for epoch in range(1, epochs+1):\n",
    "        epoch_start_time = time.time()\n",
    "        train()\n",
    "        val_loss = evaluate(val_data)\n",
    "        print('-' * 89)\n",
    "        print('| end of epoch {:3d} | time: {:5.2f}s | valid loss {:5.2f} | '\n",
    "                'valid ppl {:8.2f}'.format(epoch, (time.time() - epoch_start_time),\n",
    "                                           val_loss, math.exp(val_loss)))\n",
    "        print('-' * 89)\n",
    "        # Save the model if the validation loss is the best we've seen so far.\n",
    "        if not best_val_loss or val_loss < best_val_loss:\n",
    "            with open(args.save, 'wb') as f:\n",
    "                torch.save(model, f)\n",
    "            best_val_loss = val_loss\n",
    "        else:\n",
    "            # Anneal the learning rate if no improvement has been seen in the validation dataset.\n",
    "            lr /= 4.0\n",
    "except KeyboardInterrupt:\n",
    "    print('-' * 89)\n",
    "    print('Exiting from training early')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(0, 0)\n",
      "(1, 30)\n",
      "(2, 60)\n",
      "(3, 90)\n"
     ]
    }
   ],
   "source": [
    "for batch, i in enumerate(range(0, 100 - 1, bptt)):\n",
    "    print(batch,i)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.15"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
