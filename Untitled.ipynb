{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import argparse\n",
    "import time\n",
    "import math\n",
    "import os\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.onnx\n",
    "\n",
    "from utils import data\n",
    "from models.RNNLM import RNNModel"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "device = torch.device(\"cpu\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "corpus = data.Corpus('./data/wikitext')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([217646])\n",
      "torch.Size([217646])\n"
     ]
    }
   ],
   "source": [
    "print(corpus.valid.shape)\n",
    "print(corpus.valid.size())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def batchify(data, bsz):\n",
    "    # Work out how cleanly we can divide the dataset into bsz parts.\n",
    "    nbatch = data.size(0) // bsz\n",
    "    # Trim off any extra elements that wouldn't cleanly fit (remainders).\n",
    "    data = data.narrow(0, 0, nbatch * bsz)\n",
    "    # Evenly divide the data across the bsz batches.\n",
    "    data = data.view(bsz, -1).t().contiguous()\n",
    "    return data.to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_size = 6\n",
    "eval_batch_size = 10\n",
    "bptt = 30\n",
    "lr = 0.1\n",
    "clip = 0.25\n",
    "log_interval = 10\n",
    "epochs = 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_data = batchify(corpus.train, batch_size)\n",
    "val_data = batchify(corpus.valid, eval_batch_size)\n",
    "test_data = batchify(corpus.test, eval_batch_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def repackage_hidden(h):\n",
    "    \"\"\"Wraps hidden states in new Tensors, to detach them from their history.\"\"\"\n",
    "    if isinstance(h, torch.Tensor):\n",
    "        return h.detach()\n",
    "    else:\n",
    "        return tuple(repackage_hidden(v) for v in h)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_batch(source, i):\n",
    "    seq_len = min(bptt, len(source) - 1 - i)\n",
    "    data = source[i:i+seq_len]\n",
    "    target = source[i+1:i+1+seq_len].view(-1)\n",
    "    return data, target"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "a = get_batch(train_data,1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([30, 6])\n"
     ]
    }
   ],
   "source": [
    "print(a[0].size())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[    1,    17,  1007,   676, 60633,  4592],\n",
      "        [    2,  1640,    39,   527,   119,    15],\n",
      "        [    3, 12373,    55,   135,    17,  2059],\n",
      "        [    4, 28130,   147, 21079,   279,    43],\n",
      "        [    1,  1655,  4775,    62, 19760,    17],\n",
      "        [    0, 28294,    43,    17,    81,   932],\n",
      "        [    0,   186,  4054,  7716,   117,    13],\n",
      "        [    5,    16,    22,    62, 60621,    17],\n",
      "        [    6,    17,    17,    39,    15,   928],\n",
      "        [    2, 28295,  1188,    27,   179,   128],\n",
      "        [    7, 28296, 11616,  9084,   527,  5540],\n",
      "        [    8,    13,    48,  4338,  2484, 17175],\n",
      "        [    9,    27,    13,    15,    22,  6921],\n",
      "        [    3,  3103,  6087,     0,  2436,  2419],\n",
      "        [   10,    16,   253,     0, 60628,    17],\n",
      "        [   11, 28297,   183,     1,   119,  2307],\n",
      "        [    8, 20755,    22,     1,    17, 11191],\n",
      "        [   12,    10,  1345,     1,   279,  2348],\n",
      "        [   13,   101, 33091, 51495, 19760,    22],\n",
      "        [   14,  1311,   586,    95,   627,  6245],\n",
      "        [   15,    17, 40991,    85,    13,   371],\n",
      "        [    2,    62,    15, 28717, 60628, 18612],\n",
      "        [   16, 15667, 40990,     1,  2929,    22],\n",
      "        [   17, 28298,   677,     1,   756, 68511],\n",
      "        [   18, 28105,    17,     1,    37,    15],\n",
      "        [    7,    62,  1188,     0, 60634, 68511],\n",
      "        [   19,    19, 11616,     0,   756,   561],\n",
      "        [   13,   959,    48,   127,   800,  1899],\n",
      "        [   20,  1999,    23,  9573, 60629,  9619],\n",
      "        [   21,    16,    27,  4236,  9176,    43]])\n"
     ]
    }
   ],
   "source": [
    "print(a[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate(data_source):\n",
    "    # Turn on evaluation mode which disables dropout.\n",
    "    model.eval()\n",
    "    total_loss = 0.\n",
    "    ntokens = len(corpus.dictionary)\n",
    "    hidden = model.init_hidden(eval_batch_size)\n",
    "    with torch.no_grad():\n",
    "        for i in range(0, data_source.size(0) - 1, bptt):\n",
    "            data, targets = get_batch(data_source, i)\n",
    "            output, hidden = model(data, hidden)\n",
    "            output_flat = output.view(-1, ntokens)\n",
    "            total_loss += len(data) * criterion(output_flat, targets).item()\n",
    "            hidden = repackage_hidden(hidden)\n",
    "    return total_loss / (len(data_source) - 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train():\n",
    "    # Turn on training mode which enables dropout.\n",
    "    model.train()\n",
    "    total_loss = 0.\n",
    "    start_time = time.time()\n",
    "    ntokens = len(corpus.dictionary)\n",
    "    hidden = model.init_hidden(batch_size)\n",
    "    for batch, i in enumerate(range(0, train_data.size(0) - 1, bptt)):\n",
    "        data, targets = get_batch(train_data, i)\n",
    "        # Starting each batch, we detach the hidden state from how it was previously produced.\n",
    "        # If we didn't, the model would try backpropagating all the way to start of the dataset.\n",
    "        hidden = repackage_hidden(hidden)\n",
    "        model.zero_grad()\n",
    "        output, hidden = model(data, hidden)\n",
    "        loss = criterion(output.view(-1, ntokens), targets)\n",
    "        loss.backward()\n",
    "\n",
    "        # `clip_grad_norm` helps prevent the exploding gradient problem in RNNs / LSTMs.\n",
    "        torch.nn.utils.clip_grad_norm_(model.parameters(), clip)\n",
    "        for p in model.parameters():\n",
    "            p.data.add_(-lr, p.grad.data)\n",
    "\n",
    "        total_loss += loss.item()\n",
    "\n",
    "        if batch % log_interval == 0 and batch > 0:\n",
    "            cur_loss = total_loss / log_interval\n",
    "            elapsed = time.time() - start_time\n",
    "            print('| epoch {:3d} | {:5d}/{:5d} batches | lr {:02.2f} | ms/batch {:5.2f} | '\n",
    "                    'loss {:5.2f} | ppl {:8.2f}'.format(\n",
    "                epoch, batch, len(train_data) // bptt, lr,\n",
    "                elapsed * 1000 / log_interval, cur_loss, math.exp(cur_loss)))\n",
    "            total_loss = 0\n",
    "    start_time = time.time()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/farhood/Libs/anaconda2/lib/python2.7/site-packages/torch/nn/modules/rnn.py:46: UserWarning: dropout option adds dropout after all but last recurrent layer, so non-zero dropout expects num_layers greater than 1, but got dropout=0.5 and num_layers=1\n",
      "  \"num_layers={}\".format(dropout, num_layers))\n"
     ]
    }
   ],
   "source": [
    "ntokens = len(corpus.dictionary)\n",
    "model = RNNModel(ntokens).to(device)\n",
    "\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "\n",
    "best_val_loss = None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "| epoch   1 |    10/ 1087 batches | lr 0.10 | ms/batch 2256.37 | loss 12.48 | ppl 263284.83\n",
      "| epoch   1 |    20/ 1087 batches | lr 0.10 | ms/batch 4305.87 | loss 11.33 | ppl 83400.38\n",
      "| epoch   1 |    30/ 1087 batches | lr 0.10 | ms/batch 6366.69 | loss 11.32 | ppl 82282.31\n",
      "| epoch   1 |    40/ 1087 batches | lr 0.10 | ms/batch 8444.80 | loss 11.30 | ppl 81143.83\n",
      "| epoch   1 |    50/ 1087 batches | lr 0.10 | ms/batch 10523.16 | loss 11.29 | ppl 79959.88\n",
      "| epoch   1 |    60/ 1087 batches | lr 0.10 | ms/batch 12590.56 | loss 11.27 | ppl 78825.20\n",
      "| epoch   1 |    70/ 1087 batches | lr 0.10 | ms/batch 14657.28 | loss 11.26 | ppl 77454.02\n",
      "| epoch   1 |    80/ 1087 batches | lr 0.10 | ms/batch 16744.95 | loss 11.25 | ppl 76514.76\n",
      "| epoch   1 |    90/ 1087 batches | lr 0.10 | ms/batch 18827.69 | loss 11.23 | ppl 75292.04\n",
      "| epoch   1 |   100/ 1087 batches | lr 0.10 | ms/batch 20901.19 | loss 11.22 | ppl 74368.92\n",
      "-----------------------------------------------------------------------------------------\n",
      "Exiting from training early\n"
     ]
    }
   ],
   "source": [
    "try:\n",
    "    for epoch in range(1, epochs+1):\n",
    "        epoch_start_time = time.time()\n",
    "        train()\n",
    "        val_loss = evaluate(val_data)\n",
    "        print('-' * 89)\n",
    "        print('| end of epoch {:3d} | time: {:5.2f}s | valid loss {:5.2f} | '\n",
    "                'valid ppl {:8.2f}'.format(epoch, (time.time() - epoch_start_time),\n",
    "                                           val_loss, math.exp(val_loss)))\n",
    "        print('-' * 89)\n",
    "        # Save the model if the validation loss is the best we've seen so far.\n",
    "        if not best_val_loss or val_loss < best_val_loss:\n",
    "            with open(args.save, 'wb') as f:\n",
    "                torch.save(model, f)\n",
    "            best_val_loss = val_loss\n",
    "        else:\n",
    "            # Anneal the learning rate if no improvement has been seen in the validation dataset.\n",
    "            lr /= 4.0\n",
    "except KeyboardInterrupt:\n",
    "    print('-' * 89)\n",
    "    print('Exiting from training early')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.15"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
